{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ccfb16",
   "metadata": {},
   "source": [
    "# 02 — Data Preprocessing (Triple Pipeline)\n",
    "\n",
    "This notebook prepares THREE variations of the dataset:\n",
    "1. **Standard**: Basic cleaning (lowercase, remove URLs/emojis).\n",
    "2. **Irony-Augmented**: Standard cleaning + `[IRONIA]` tagging for detected colloquialisms.\n",
    "3. **Obfuscated**: Standard cleaning + NER-based obfuscation of people names to `[PERSONA]`.\n",
    "\n",
    "**Output Locations**:\n",
    "- `../data/processed/standard/`\n",
    "- `../data/processed/irony/`\n",
    "- `../data/processed/obfuscated/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25d27b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:36.587592Z",
     "iopub.status.busy": "2026-02-15T20:40:36.587196Z",
     "iopub.status.idle": "2026-02-15T20:40:40.148658Z",
     "shell.execute_reply": "2026-02-15T20:40:40.147536Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Ensure data directories exist\n",
    "os.makedirs('../data/processed/standard', exist_ok=True)\n",
    "os.makedirs('../data/processed/irony', exist_ok=True)\n",
    "os.makedirs('../data/processed/obfuscated', exist_ok=True)\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc07b083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:40.154176Z",
     "iopub.status.busy": "2026-02-15T20:40:40.153723Z",
     "iopub.status.idle": "2026-02-15T20:40:40.198378Z",
     "shell.execute_reply": "2026-02-15T20:40:40.196978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.0\n",
      "IPython version      : 9.10.0\n",
      "\n",
      "numpy  : 2.4.2\n",
      "pandas : 3.0.0\n",
      "sklearn: 1.8.0\n",
      "emoji  : 2.15.0\n",
      "\n",
      "Compiler    : Clang 16.0.0 (clang-1600.0.26.4)\n",
      "OS          : Darwin\n",
      "Release     : 25.2.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,pandas,sklearn,emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238791c",
   "metadata": {},
   "source": [
    "## 1. Load Data & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18205725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:40.204125Z",
     "iopub.status.busy": "2026-02-15T20:40:40.203700Z",
     "iopub.status.idle": "2026-02-15T20:40:40.281210Z",
     "shell.execute_reply": "2026-02-15T20:40:40.280175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 samples\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/corpus.csv')\n",
    "df['quote_safe'] = df['QuoteText'].fillna('')\n",
    "df['text'] = df.apply(lambda x: (x['quote_safe'] + \" \" + x['TweetText']).strip(), axis=1)\n",
    "df['label'] = df['Categorization']\n",
    "df = df[['text', 'label']]\n",
    "print(f\"Loaded {df.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "obfuscation_logic",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:40.286082Z",
     "iopub.status.busy": "2026-02-15T20:40:40.285711Z",
     "iopub.status.idle": "2026-02-15T20:40:42.617426Z",
     "shell.execute_reply": "2026-02-15T20:40:42.616484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: es-core-news-lg\n",
      "Successfully installed es-core-news-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_lg\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"es_core_news_lg\")\n",
    "    nlp = spacy.load(\"es_core_news_lg\")\n",
    "\n",
    "def strip_obfuscation_tags(text):\n",
    "    \"\"\"Remove existing obfuscation tags like [ANATOMIA], [GROSERIA], etc. before NER.\"\"\"\n",
    "    return re.sub(r'\\[([A-Z_]+)\\]', '', text)\n",
    "\n",
    "def is_valid_name(ent):\n",
    "    \"\"\"\n",
    "    Check if a detected PER entity is likely a false positive.\n",
    "    Returns True if it seems like a valid name, False if it's likely a common word.\n",
    "    Strategy: Check the POS tag of the lowercased tokens in isolation.\n",
    "    \"\"\"\n",
    "    # POS tags that clearly shouldn't be part of a person's name in this context\n",
    "    # We include NOUN because common nouns (el \"drogas\") shouldn't be obfuscated as PERSONA ideally, \n",
    "    # but 'gil' is PROPN so it slips through if we only check NOUN. \n",
    "    # However, 'Rindo' is VERB, 'mitotero' is ADJ.\n",
    "    invalid_pos = {'VERB', 'AUX', 'ADJ', 'ADV', 'INTJ', 'PRON', 'DET', 'CONJ', 'NUM', 'SCONJ'}\n",
    "    \n",
    "    for token in ent:\n",
    "        # Check the token in lower case and isolation\n",
    "        # This helps check if the word *can* be a common word\n",
    "        doc_lower = nlp.make_doc(token.text.lower())\n",
    "        # We need to run the tagger on this single token doc\n",
    "        # Just running nlp() is safer to get the pipeline's opinion\n",
    "        doc_lower = nlp(token.text.lower())\n",
    "        token_lower = doc_lower[0]\n",
    "        \n",
    "        if token_lower.pos_ in invalid_pos:\n",
    "            return False\n",
    "            \n",
    "    return True\n",
    "\n",
    "def obfuscate_entities(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    # Strip existing obfuscation tags so NER doesn't pick them up as names\n",
    "    cleaned_for_ner = strip_obfuscation_tags(text)\n",
    "    \n",
    "    # Run NER\n",
    "    doc = nlp(cleaned_for_ner)\n",
    "    \n",
    "    # Identify valid entities to replace\n",
    "    entities_to_replace = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER' and is_valid_name(ent):\n",
    "            entities_to_replace.append(ent.text)\n",
    "            \n",
    "    # Replace in the original text\n",
    "    for entity_text in set(entities_to_replace): # Use set to avoid double work\n",
    "        # Escape regex special characters in the name\n",
    "        pattern = r'\\b' + re.escape(entity_text) + r'\\b'\n",
    "        text = re.sub(pattern, '[PERSONA]', text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def process_obfuscated(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # 1. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # 2. Demojize\n",
    "    text = emoji.demojize(text, language='es', delimiters=(\" :\", \": \"))\n",
    "    # 3. Obfuscate People Names ([PERSONA])\n",
    "    text = obfuscate_entities(text)\n",
    "    # 4. Lowercase and clean whitespace\n",
    "    # Lowercase EXCEPT tags (preserves [ANATOMIA], [PERSONA], etc.)\n",
    "    parts = re.split(r'(\\[[A-ZÁÉÍÓÚÑ]+\\])', text)\n",
    "    processed = []\n",
    "    for part in parts:\n",
    "        if re.match(r'^\\[[A-ZÁÉÍÓÚÑ]+\\]$', part):\n",
    "            processed.append(part)\n",
    "        else:\n",
    "            processed.append(part.lower())\n",
    "    text = \"\".join(processed)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaba4d55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:42.622000Z",
     "iopub.status.busy": "2026-02-15T20:40:42.621514Z",
     "iopub.status.idle": "2026-02-15T20:40:42.630949Z",
     "shell.execute_reply": "2026-02-15T20:40:42.630057Z"
    }
   },
   "outputs": [],
   "source": [
    "def tag_irony_logic(text):\n",
    "    if not isinstance(text, str): return text\n",
    "    # Laughs\n",
    "    text = re.sub(r'(?i)\\b(j+a+){2,}\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\b(j+e+){2,}\\b', ' [IRONIA] ', text)\n",
    "    # Specific phrases\n",
    "    text = re.sub(r'\\(\\?+\\)?', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bx+d+\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\b(a+h? ?r+e+)\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bare\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bbue\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bwe\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bbueno no\\b', ' [IRONIA] ', text)\n",
    "    text = re.sub(r'(?i)\\bno bueno\\b', ' [IRONIA] ', text)\n",
    "    return text\n",
    "\n",
    "def clean_base(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # Demojize\n",
    "    text = emoji.demojize(text, language='es', delimiters=(\" :\", \": \"))\n",
    "    # Lowercase EXCEPT tags\n",
    "    parts = re.split(r'(\\[[A-ZÁÉÍÓÚÑ]+\\])', text)\n",
    "    processed = []\n",
    "    for part in parts:\n",
    "        if re.match(r'^\\[[A-ZÁÉÍÓÚÑ]+\\]$', part):\n",
    "            processed.append(part)\n",
    "        else:\n",
    "            processed.append(part.lower())\n",
    "    text = \"\".join(processed)\n",
    "    # Whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def process_standard(text):\n",
    "    return clean_base(text)\n",
    "\n",
    "def process_irony(text):\n",
    "    # Tag irony FIRST, then clean (so [IRONIA] is preserved as uppercase tag)\n",
    "    text = tag_irony_logic(text)\n",
    "    return clean_base(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d75e3d",
   "metadata": {},
   "source": [
    "## 2. Generate Datasets\n",
    "\n",
    "We now apply the preprocessing pipelines to generate three distinct datasets:\n",
    "*   **Standard**: Baseline for model performance.\n",
    "*   **Irony**: To test if explicit irony tagging helps the model.\n",
    "*   **Obfuscated**: To test if masking personal names (`[PERSONA]`) improves generalization or reduces bias.\n",
    "\n",
    "> **Note**: The `Obfuscated` pipeline uses the `es_core_news_lg` model for better accuracy in detecting people's names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed42d380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:40:42.636025Z",
     "iopub.status.busy": "2026-02-15T20:40:42.635296Z",
     "iopub.status.idle": "2026-02-15T20:41:16.968985Z",
     "shell.execute_reply": "2026-02-15T20:41:16.967547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc51e7349ae4cd985b43ad43f36033c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard\n",
    "df_standard = df.copy()\n",
    "df_standard['text_clean'] = df_standard['text'].apply(process_standard)\n",
    "\n",
    "# Irony\n",
    "df_irony = df.copy()\n",
    "df_irony['text_clean'] = df_irony['text'].apply(process_irony)\n",
    "\n",
    "# Obfuscated\n",
    "df_obfuscated = df.copy()\n",
    "df_obfuscated['text_clean'] = df_obfuscated['text'].progress_apply(process_obfuscated)\n",
    "\n",
    "print(\"Sample Standard:\", df_standard['text_clean'].iloc[10])\n",
    "print(\"Sample Irony:   \", df_irony['text_clean'].iloc[10])\n",
    "print(\"Sample Obfuscated:\", df_obfuscated['text_clean'].iloc[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obfuscation_examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Before & After Examples for Obfuscation\n",
    "print(\"\\n=== Obfuscation Examples (Focus on [PERSONA]) ===\\n\")\n",
    "\n",
    "# specific logic to find relevant examples\n",
    "persona_mask = df_obfuscated['text_clean'].str.contains(r'\\[PERSONA\\]', regex=True)\n",
    "\n",
    "if persona_mask.sum() > 0:\n",
    "    # Show up to 5 examples where PERSONA was inserted\n",
    "    examples = df_obfuscated[persona_mask].sample(min(5, persona_mask.sum()), random_state=42)\n",
    "    for idx, row in examples.iterrows():\n",
    "        original = df.loc[idx, 'text']\n",
    "        processed = row['text_clean']\n",
    "        print(f\"Original:  {original}\")\n",
    "        print(f\"Processed: {processed}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No [PERSONA] tags found in the processed text.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac327a5",
   "metadata": {},
   "source": [
    "## 3. Split and Save\n",
    "\n",
    "Finally, we split each dataset into Train (70%), Validation (15%), and Test (15%) sets.\n",
    "All splits are stratified by label to ensure balanced class distribution.\n",
    "\n",
    "Files are saved to:\n",
    "*   `../data/processed/standard/`\n",
    "*   `../data/processed/irony/`\n",
    "*   `../data/processed/obfuscated/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c15df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T20:41:16.974711Z",
     "iopub.status.busy": "2026-02-15T20:41:16.973992Z",
     "iopub.status.idle": "2026-02-15T20:41:17.118199Z",
     "shell.execute_reply": "2026-02-15T20:41:17.117108Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_splits(dataframe, name, output_dir):\n",
    "    train, temp = train_test_split(dataframe, test_size=0.3, stratify=dataframe['label'], random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, stratify=temp['label'], random_state=42)\n",
    "    \n",
    "    train.to_csv(f'{output_dir}/train.csv', index=False)\n",
    "    val.to_csv(f'{output_dir}/val.csv', index=False)\n",
    "    test.to_csv(f'{output_dir}/test.csv', index=False)\n",
    "    print(f\"Saved {name} splits to {output_dir}\")\n",
    "\n",
    "save_splits(df_standard, \"Standard\", \"../data/processed/standard\")\n",
    "save_splits(df_irony, \"Irony\", \"../data/processed/irony\")\n",
    "save_splits(df_obfuscated, \"Obfuscated\", \"../data/processed/obfuscated\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
